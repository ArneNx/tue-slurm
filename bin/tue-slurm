#!/usr/bin/env python3

import argparse
import os
import stat
import subprocess
from pathlib import Path
from time import sleep


class SlurmJob:
    def __init__(
        self,
        name,
        time,
        gpu,
        num_gpus,
        memory,
        email,
        on_local,
        run_script,
        slurm_img,
        unk_args,
        index=0,
    ):
        host_name = os.environ.get("HOSTNAME")
        self.on_tue_cluster = "bg-slurmb" in host_name

        self.name = f"{name}-{index}"
        self.email = email
        self.time = time

        days, hours, minutes = list(
            map(int, [time.split("-")[0]] + time.split("-")[1].split(":"))
        )
        if self.on_tue_cluster:
            if "2080" in gpu:
                self.gpu = "gpu-2080ti-dev" if hours < 12 else "gpu-2080ti"
            else:
                self.gpu = "gpu-v100"
        else:
            self.gpu = gpu  # "gtx1080", "v100", "gtx1080"
            self.partition = "gpu"
            self.scratch = "scratch2" if gpu in ("gtx1080", "gtx980") else "scratch"
        self.num_gpus = num_gpus
        self.memory = memory
        self.on_slurm = not on_local
        self.run_script = run_script
        self.run_args = " ".join(unk_args)
        self.slurm_img = slurm_img

    @property
    def resource_config_string(self):

        if not Path("logs").exists():
            os.mkdir("logs")
        config_string = f"""
#SBATCH --job-name={self.name}                   # Name of the job
#SBATCH --ntasks=1                          # Number of tasks
#SBATCH --cpus-per-task=2                   # Number of CPU cores per task
#SBATCH --nodes=1                           # Ensure that all cores are on one machine
#SBATCH --time={self.time}                       # Runtime in D-HH:MM
#SBATCH --mem-per-cpu={self.memory}              # Memory pool for all cores (see also --mem-per-cpu)
#SBATCH --output=logs/{self.name}.%j.out              # File to which STDOUT will be written
#SBATCH --error=logs/{self.name}.%j.err               # File to which STDERR will be written
#SBATCH --mail-type=ALL                     # Type of email notification- BEGIN,END,FAIL,ALL
#SBATCH --mail-user={self.email}                 # Email to which notifications will be sent
"""
        if self.on_tue_cluster:
            config_string += f"""
#SBATCH --partition={self.gpu}                   # Partition to submit to
#SBATCH --gres=gpu:{self.num_gpus}
            """
        else:
            config_string += f"""
#SBATCH --constraint="{self.scratch}"                # Mount scratch
#SBATCH -p {self.partition}                   # Partition to submit to
#SBATCH --gpus={self.gpu}:{self.num_gpus}               # Number of requested GPUs
            """
        return config_string

    @property
    def singularity_run_command(self):
        if self.on_tue_cluster:
            run_cmd = f"""
            """
        else:
            run_cmd = f"""
module purge
module load singularity
module load cuda

# export SCRATCH=/scratch/users/$USER

#Setting up the temporary scratch directory:
export SCRATCH=/scratch/users/$USER/$SLURM_JOB_ID
mkdir -p $SCRATCH
            """

        run_cmd += f"""
scontrol show job $SLURM_JOB_ID  # print some info

singularity exec \
--nv \
--env-file .env \
--no-home  \
--bind $SCRATCH:$SCRATCH,$HOME/projects/:$HOME/projects/,$HOME/src/:$HOME/src/,$HOME/.local/:$HOME/.local/  \
{self.slurm_img} \
{self.run_script} {self.run_args}

# Remove the temporary scratch dir
rm -rf $SCRATCH
        """
        return run_cmd

    def run(self):
        if self.on_slurm:
            slurm_job_bash_file = f"./{self.name}.sh"
            slurm_job_bash_file_content = (
                "#!/bin/bash \n \n"
                + self.resource_config_string
                + "\n"
                + self.singularity_run_command
            )
            with open(slurm_job_bash_file, "w") as f:
                f.write(slurm_job_bash_file_content)

            os.chmod(slurm_job_bash_file, stat.S_IRWXU)

            try:
                output = subprocess.check_output(
                    "sbatch " + slurm_job_bash_file, shell=True
                )
                sleep(5)
                job_id = int(output[20:].strip())
                node = subprocess.check_output(
                    f"scontrol show job {job_id}| grep ' NodeList'", shell=True
                ).strip()
                node = str(node).split("=")[1][:-1]
                print(f"Successfully submitted job with ID {job_id} to node {node}.")
            finally:
                # remove the bash file
                os.remove(slurm_job_bash_file)
        else:
            print(subprocess.check_output(self.singularity_run_command, shell=True))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Running jobs on SLURM cluster")
    parser.add_argument(
        "--run",
        dest="run_script",
        action="store",
        default="run.py",
        type=str,
        help="",
    )
    parser.add_argument(
        "--njobs",
        dest="num_jobs",
        action="store",
        default=1,
        type=int,
        help="",
    )
    parser.add_argument(
        "--name",
        dest="name",
        action="store",
        default="noname",
        type=str,
        help="",
    )
    parser.add_argument(
        "--time",
        dest="time",
        action="store",
        default="0-01:00",
        type=str,
        help="time to complete each job. Specify in the following format: D-HH:MM",
    )
    parser.add_argument(
        "--gpu",
        dest="gpu",
        action="store",
        default="gpu-2080ti",
        type=str,
        help="",
    )
    parser.add_argument(
        "--ngpus",
        dest="num_gpus",
        action="store",
        default=1,
        type=int,
        help="",
    )
    parser.add_argument(
        "--memory",
        dest="memory",
        action="store",
        default=3000,
        type=int,
        help="",
    )
    parser.add_argument(
        "--email",
        dest="email",
        action="store",
        default=os.getenv("EMAIL"),
        type=str,
        help="",
    )
    parser.add_argument(
        "--local",
        dest="local",
        default=False,
        action="store_true",
        help="Specify whether this is a job on SLURM or a local machine.",
    )
    parser.add_argument(
        "--img",
        dest="slurm_img",
        action="store",
        default="shub://sinzlab/pytorch-singularity:v3.8-torch1.7.0-dj0.12.7",
        type=str,
        help="Singularity image to use",
    )

    args, unk_args = parser.parse_known_args()

    for job_index in range(args.num_jobs):
        job = SlurmJob(
            args.name,
            args.time,
            args.gpu,
            args.num_gpus,
            args.memory,
            args.email,
            args.local,
            args.run_script,
            args.slurm_img,
            unk_args,
            index=job_index,
        )
        job.run()
